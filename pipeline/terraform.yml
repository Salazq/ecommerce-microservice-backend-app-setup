# Este pipeline se debe disparar manualmente.
trigger: none

pool:
  vmImage: ubuntu-latest

parameters:
- name: environment
  displayName: 'Select Environment'
  type: string
  default: 'dev'
  values:
  - dev
  - staging
  - prod

variables:
  - group: variable-group-taller
  - name: environmentPath
    value: 'terraform/environments/${{ parameters.environment }}'
  - name: aksResourceGroup
    value: '${{ parameters.environment }}-resource-group'
  - name: aksClusterName
    value: '${{ parameters.environment }}-aks-cluster'

stages:
- stage: Terraform_plan_apply_${{ parameters.environment }}
  displayName: 'Terraform Plan & Apply - ${{ parameters.environment }}'
  # Etapa √∫nica: Generar el plan y aplicar Terraform.
  jobs:
    - job: PlanAndApply
      continueOnError: false
      timeoutInMinutes: 30
      steps:
        # Paso 1: Instalar Terraform
        - task: TerraformInstaller@0
          inputs:
            terraformVersion: '1.5.7'
          displayName: 'Install Terraform'

        # Paso 2: Verificar versi√≥n
        - script: terraform --version
          displayName: 'Check Terraform version'        
          # Paso 3: Verificar estructura de directorios
        - script: |
            echo "Working with environment: ${{ parameters.environment }}"
            echo "Environment path: $(environmentPath)"
            echo "Current directory: $(System.DefaultWorkingDirectory)"
            ls -la $(System.DefaultWorkingDirectory)/ || dir $(System.DefaultWorkingDirectory)\
            ls -la $(System.DefaultWorkingDirectory)/$(environmentPath)/ || dir $(System.DefaultWorkingDirectory)\$(environmentPath)\ || echo "Directory not found"
            echo "Files in environment directory:"
            ls -la $(System.DefaultWorkingDirectory)/$(environmentPath)/*.tf $(System.DefaultWorkingDirectory)/$(environmentPath)/*.tfvars 2>/dev/null || dir $(System.DefaultWorkingDirectory)\$(environmentPath)\*.tf $(System.DefaultWorkingDirectory)\$(environmentPath)\*.tfvars 2>nul || echo "No .tf or .tfvars files found"
          displayName: 'Check directory structure and files'
        # Paso 4: Terraform init
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'init'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            backendServiceArm: $(AZURE_ACCOUNT)
            backendAzureRmResourceGroupName: 'VM'
            backendAzureRmStorageAccountName: 'vmrecursos'
            backendAzureRmContainerName: 'tfstate'
            backendAzureRmKey: '${{ parameters.environment }}-aks.tfstate'
          displayName: 'Terraform init'

        # Paso 5: Terraform validate
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'validate'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
          displayName: 'Terraform validate'        # Paso 6: Terraform plan (usando task oficial)
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'plan'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            environmentServiceNameAzureRM: $(AZURE_ACCOUNT)
            commandOptions: '-out=plan.out -var-file=terraform.tfvars'
          displayName: 'Terraform Plan'

        # Paso 7: Publicar artefacto del plan
        - task: PublishPipelineArtifact@1
          inputs:
            targetPath: '$(System.DefaultWorkingDirectory)/$(environmentPath)/plan.out'
            artifact: 'Plan-${{ parameters.environment }}'
            publishLocation: 'pipeline'
          displayName: 'Publish Plan Artifact'
          condition: succeeded()

        # Paso 8: Terraform apply
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'apply'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            environmentServiceNameAzureRM: $(AZURE_ACCOUNT)
            commandOptions: '-auto-approve plan.out'
          displayName: 'Terraform Apply - ${{ parameters.environment }}'        # Paso 9: Mostrar outputs
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'output'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            environmentServiceNameAzureRM: $(AZURE_ACCOUNT)
          displayName: 'Show Terraform Outputs'
          continueOnError: true

        # Paso 10: Validar deployment
        - script: |
            echo "Deployment completed for environment: ${{ parameters.environment }}"
            echo "Validating deployment..."
            cd $(System.DefaultWorkingDirectory)/$(environmentPath)
            # Verificar estado de Terraform
            terraform show -json > terraform_state.json 2>/dev/null || echo "Could not export state"
            # Mostrar resumen de recursos creados
            terraform state list 2>/dev/null || echo "Could not list state"
            echo "Deployment validation completed"
          displayName: 'Validate Deployment'
          continueOnError: true

        # Paso 11: Cleanup temporal files
        - script: |
            cd $(System.DefaultWorkingDirectory)/$(environmentPath)
            # Limpiar archivos temporales pero conservar el estado
            rm -f plan.out terraform_state.json 2>/dev/null || del plan.out terraform_state.json 2>nul
            echo "Cleanup completed"
          displayName: 'Cleanup temporary files'
          continueOnError: true
          condition: always()

- stage: DeployMonitoringStack
  displayName: 'Instalar Prometheus & Grafana con Helm'
  dependsOn: Terraform_plan_apply_${{ parameters.environment }}
  condition: succeeded()
  jobs:
    - job: InstallMonitoring
      displayName: 'Helm Install kube-prometheus-stack'
      pool:
        vmImage: 'ubuntu-latest'
      steps:
        - task: HelmInstaller@1
          inputs:
            helmVersionToInstall: '3.13.3'
        - task: AzureCLI@2
          displayName: 'Login en AKS'
          inputs:
            azureSubscription:  $(AZURE_ACCOUNT)  # Debe estar configurado en Azure DevOps
            scriptType: 'bash'
            scriptLocation: 'inlineScript'
            inlineScript: |
              az aks get-credentials --resource-group $(aksResourceGroup) --name $(aksClusterName) --overwrite-existing

        - script: |
            echo "üîß Crear namespace monitoring si no existe"
            kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

            echo "üì¶ Agregar repositorio Helm de Prometheus"
            helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
            helm repo update

            echo "üöÄ Instalar kube-prometheus-stack con configuraci√≥n para Nginx"
            helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              --namespace monitoring \
              --set grafana.adminPassword='MiPassword123' \
              --set grafana.service.type=ClusterIP \
              --set grafana.env.GF_SERVER_ROOT_URL='http://localhost/grafana/' \
              --set prometheus.service.type=ClusterIP \
              --set prometheus.prometheusSpec.externalUrl='http://localhost/prometheus/'\
              --set prometheus.prometheusSpec.routePrefix='/prometheus'
          displayName: 'Instalar Prometheus y Grafana en monitoring'


- stage: DeployLoggingStack
  displayName: 'Instalar ELK Stack (Elasticsearch, Logstash, Kibana)'
  dependsOn: DeployMonitoringStack
  condition: succeeded()
  jobs:
    - job: InstallELK
      displayName: 'Instalar Elasticsearch, Kibana y Logstash con Helm'
      pool:
        vmImage: 'ubuntu-latest'
      steps:
        - task: HelmInstaller@1
          inputs:
            helmVersionToInstall: '3.13.3'
        - task: AzureCLI@2
          displayName: 'Login en AKS'
          inputs:
            azureSubscription: $(AZURE_ACCOUNT)
            scriptType: 'bash'
            scriptLocation: 'inlineScript'
            inlineScript: |
              az aks get-credentials --resource-group $(aksResourceGroup) --name $(aksClusterName) --overwrite-existing
        - script: |
            echo "üõ† Crear namespace logging si no existe"
            kubectl create namespace logging --dry-run=client -o yaml | kubectl apply -f -
            
            echo "üßπ Limpiar instalaciones previas completamente"
            helm uninstall elasticsearch -n logging --ignore-not-found
            helm uninstall kibana -n logging --ignore-not-found  
            helm uninstall logstash -n logging --ignore-not-found
            kubectl delete configmap kibana-kibana-helm-scripts -n logging --ignore-not-found
            kubectl delete pvc -n logging --all --ignore-not-found
            kubectl delete pv --all --ignore-not-found
            
            # Esperar a que se limpien los recursos
            sleep 10
            
            echo "üîê Crear secreto con contrase√±a de elastic"
            kubectl delete secret elastic-credentials -n logging --ignore-not-found
            kubectl create secret generic elastic-credentials --from-literal=password='changeme123' -n logging
            
            echo "üì¶ Agregar repositorio Helm Elastic"
            helm repo add elastic https://helm.elastic.co
            helm repo update
            
            echo "üîç Verificar StorageClasses disponibles"
            kubectl get storageclass -o wide
            
            echo "üîç Verificar nodos disponibles"
            kubectl get nodes -o wide
            
            echo "üöÄ Instalar Elasticsearch con configuraci√≥n simplificada"
            helm install elasticsearch elastic/elasticsearch \
              --namespace logging \
              --set replicas=1 \
              --set minimumMasterNodes=1 \
              --set volumeClaimTemplate.storageClassName="default" \
              --set volumeClaimTemplate.resources.requests.storage=2Gi \
              --set volumeClaimTemplate.accessModes[0]="ReadWriteOnce" \
              --set persistence.enabled=true \
              --set secret.enabled=false \
              --set security.enabled=false \
              --set protocol=http \
              --set httpPort=9200 \
              --set transportPort=9300 \
              --set resources.requests.cpu=100m \
              --set resources.requests.memory=1Gi \
              --set resources.limits.cpu=500m \
              --set resources.limits.memory=2Gi \
              --set esJavaOpts="-Xms512m -Xmx512m" \
              --set sysctlInitContainer.enabled=false \
              --set readinessProbe.initialDelaySeconds=60 \
              --set readinessProbe.periodSeconds=30 \
              --set readinessProbe.timeoutSeconds=10 \
              --set readinessProbe.failureThreshold=10 \
              --set clusterHealthCheckParams="wait_for_status=yellow&timeout=30s" \
              --timeout=15m \
              --wait
              
            echo "üîç Estado de Elasticsearch despu√©s de la instalaci√≥n:"
            kubectl get pods -n logging -l app=elasticsearch-master -o wide
            kubectl get pvc -n logging
            kubectl get events -n logging --sort-by='.lastTimestamp' | tail -10
            
            # Verificar que Elasticsearch est√© funcionando antes de continuar
            echo "‚è≥ Esperando a que Elasticsearch est√© completamente listo..."
            kubectl wait --for=condition=ready pod -l app=elasticsearch-master -n logging --timeout=900s
            
            # Verificar conectividad
            echo "üîç Verificar conectividad de Elasticsearch:"
            kubectl exec -n logging deployment/elasticsearch-master -- curl -s http://localhost:9200/_cluster/health || echo "‚ö†Ô∏è Elasticsearch no responde a√∫n"
            
            echo "üöÄ Instalar Kibana"
            helm install kibana elastic/kibana \
              --namespace logging \
              --set elasticsearchHosts="http://elasticsearch-master:9200" \
              --set service.type=ClusterIP \
              --set resources.requests.cpu=100m \
              --set resources.requests.memory=512Mi \
              --set resources.limits.cpu=500m \
              --set resources.limits.memory=1Gi \
              --set readinessProbe.initialDelaySeconds=60 \
              --set readinessProbe.periodSeconds=30 \
              --set readinessProbe.timeoutSeconds=10 \
              --set readinessProbe.failureThreshold=10 \
              --timeout=10m \
              --wait
              
            echo "üìù Crear archivo de configuraci√≥n para Logstash"
            cat <<EOF > values-logstash.yaml
            logstashPipeline:
              logstash.conf: |
                input {
                  beats {
                    port => 5044
                  }
                }
                filter {
                  if [fields][service] {
                    mutate {
                      add_field => { "service_name" => "%{[fields][service]}" }
                    }
                  }
                }
                output {
                  elasticsearch {
                    hosts => ["http://elasticsearch-master:9200"]
                    index => "logs-%{+YYYY.MM.dd}"
                  }
                  stdout {
                    codec => rubydebug
                  }
                }
            extraEnvs:
              - name: LS_JAVA_OPTS
                value: "-Xms256m -Xmx256m"
            resources:
              requests:
                cpu: 100m
                memory: 512Mi
              limits:
                cpu: 500m
                memory: 1Gi
            logstashJavaOpts: "-Xms256m -Xmx256m"
            service:
              type: ClusterIP
              ports:
                - name: beats
                  port: 5044
                  protocol: TCP
                  targetPort: 5044
            replicas: 1
            readinessProbe:
              httpGet:
                path: /
                port: 9600
              initialDelaySeconds: 60
              periodSeconds: 30
              timeoutSeconds: 10
              failureThreshold: 10
            livenessProbe:
              httpGet:
                path: /
                port: 9600
              initialDelaySeconds: 120
              periodSeconds: 30
              timeoutSeconds: 10
              failureThreshold: 5
            EOF
            
            echo "üöÄ Instalar Logstash"
            helm install logstash elastic/logstash \
              --namespace logging \
              -f values-logstash.yaml \
              --timeout=10m \
              --wait
              
            echo "‚úÖ Verificar el estado final de todos los pods"
            kubectl get pods -n logging -o wide
            kubectl get svc -n logging
            kubectl get pvc -n logging
            
            echo "üîç Verificar logs recientes si hay problemas:"
            echo "Para Elasticsearch: kubectl logs -n logging -l app=elasticsearch-master --tail=20"
            echo "Para Kibana: kubectl logs -n logging -l app=kibana --tail=20"
            echo "Para Logstash: kubectl logs -n logging -l app=logstash --tail=20"
            
            echo "üìã Informaci√≥n de conexi√≥n:"
            echo "- Elasticsearch: http://elasticsearch-master.logging.svc.cluster.local:9200"
            echo "- Kibana: http://kibana-kibana.logging.svc.cluster.local:5601"
            echo "- Logstash: logstash-logstash-headless.logging.svc.cluster.local:5044"
            
            echo "üéâ Instalaci√≥n del ELK Stack completada!"
            
          displayName: 'Desplegar ELK Stack'