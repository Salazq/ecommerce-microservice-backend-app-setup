# Este pipeline se debe disparar manualmente.
trigger: none

pool:
  vmImage: ubuntu-latest

parameters:
- name: environment
  displayName: 'Select Environment'
  type: string
  default: 'dev'
  values:
  - dev
  - staging
  - prod

variables:
  - group: variable-group-taller
  - name: environmentPath
    value: 'terraform/environments/${{ parameters.environment }}'
  - name: aksResourceGroup
    value: '${{ parameters.environment }}-resource-group'
  - name: aksClusterName
    value: '${{ parameters.environment }}-aks-cluster'

stages:
- stage: Terraform_plan_apply_${{ parameters.environment }}
  displayName: 'Terraform Plan & Apply - ${{ parameters.environment }}'
  # Etapa 煤nica: Generar el plan y aplicar Terraform.
  jobs:
    - job: PlanAndApply
      continueOnError: false
      timeoutInMinutes: 30
      steps:
        # Paso 1: Instalar Terraform
        - task: TerraformInstaller@0
          inputs:
            terraformVersion: '1.5.7'
          displayName: 'Install Terraform'

        # Paso 2: Verificar versi贸n
        - script: terraform --version
          displayName: 'Check Terraform version'        
          # Paso 3: Verificar estructura de directorios
        - script: |
            echo "Working with environment: ${{ parameters.environment }}"
            echo "Environment path: $(environmentPath)"
            echo "Current directory: $(System.DefaultWorkingDirectory)"
            ls -la $(System.DefaultWorkingDirectory)/ || dir $(System.DefaultWorkingDirectory)\
            ls -la $(System.DefaultWorkingDirectory)/$(environmentPath)/ || dir $(System.DefaultWorkingDirectory)\$(environmentPath)\ || echo "Directory not found"
            echo "Files in environment directory:"
            ls -la $(System.DefaultWorkingDirectory)/$(environmentPath)/*.tf $(System.DefaultWorkingDirectory)/$(environmentPath)/*.tfvars 2>/dev/null || dir $(System.DefaultWorkingDirectory)\$(environmentPath)\*.tf $(System.DefaultWorkingDirectory)\$(environmentPath)\*.tfvars 2>nul || echo "No .tf or .tfvars files found"
          displayName: 'Check directory structure and files'
        # Paso 4: Terraform init
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'init'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            backendServiceArm: $(AZURE_ACCOUNT)
            backendAzureRmResourceGroupName: 'VM'
            backendAzureRmStorageAccountName: 'vmrecursos'
            backendAzureRmContainerName: 'tfstate'
            backendAzureRmKey: '${{ parameters.environment }}-aks.tfstate'
          displayName: 'Terraform init'

        # Paso 5: Terraform validate
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'validate'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
          displayName: 'Terraform validate'        # Paso 6: Terraform plan (usando task oficial)
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'plan'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            environmentServiceNameAzureRM: $(AZURE_ACCOUNT)
            commandOptions: '-out=plan.out -var-file=terraform.tfvars'
          displayName: 'Terraform Plan'

        # Paso 7: Publicar artefacto del plan
        - task: PublishPipelineArtifact@1
          inputs:
            targetPath: '$(System.DefaultWorkingDirectory)/$(environmentPath)/plan.out'
            artifact: 'Plan-${{ parameters.environment }}'
            publishLocation: 'pipeline'
          displayName: 'Publish Plan Artifact'
          condition: succeeded()

        # Paso 8: Terraform apply
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'apply'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            environmentServiceNameAzureRM: $(AZURE_ACCOUNT)
            commandOptions: '-auto-approve plan.out'
          displayName: 'Terraform Apply - ${{ parameters.environment }}'        # Paso 9: Mostrar outputs
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'output'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            environmentServiceNameAzureRM: $(AZURE_ACCOUNT)
          displayName: 'Show Terraform Outputs'
          continueOnError: true

        # Paso 10: Validar deployment
        - script: |
            echo "Deployment completed for environment: ${{ parameters.environment }}"
            echo "Validating deployment..."
            cd $(System.DefaultWorkingDirectory)/$(environmentPath)
            # Verificar estado de Terraform
            terraform show -json > terraform_state.json 2>/dev/null || echo "Could not export state"
            # Mostrar resumen de recursos creados
            terraform state list 2>/dev/null || echo "Could not list state"
            echo "Deployment validation completed"
          displayName: 'Validate Deployment'
          continueOnError: true

        # Paso 11: Cleanup temporal files
        - script: |
            cd $(System.DefaultWorkingDirectory)/$(environmentPath)
            # Limpiar archivos temporales pero conservar el estado
            rm -f plan.out terraform_state.json 2>/dev/null || del plan.out terraform_state.json 2>nul
            echo "Cleanup completed"
          displayName: 'Cleanup temporary files'
          continueOnError: true
          condition: always()

- stage: DeployMonitoringStack
  displayName: 'Instalar Prometheus & Grafana con Helm'
  dependsOn: Terraform_plan_apply_${{ parameters.environment }}
  condition: succeeded()
  jobs:
    - job: InstallMonitoring
      displayName: 'Helm Install kube-prometheus-stack'
      pool:
        vmImage: 'ubuntu-latest'
      steps:
        - task: HelmInstaller@1
          inputs:
            helmVersionToInstall: '3.13.3'
        - task: AzureCLI@2
          displayName: 'Login en AKS'
          inputs:
            azureSubscription:  $(AZURE_ACCOUNT)  # Debe estar configurado en Azure DevOps
            scriptType: 'bash'
            scriptLocation: 'inlineScript'
            inlineScript: |
              az aks get-credentials --resource-group $(aksResourceGroup) --name $(aksClusterName) --overwrite-existing

        - script: |
            echo " Crear namespace monitoring si no existe"
            kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

            echo " Agregar repositorio Helm de Prometheus"
            helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
            helm repo update

            echo " Instalar kube-prometheus-stack con configuraci贸n para Nginx"
            helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              --namespace monitoring \
              --set grafana.adminPassword='MiPassword123' \
              --set grafana.service.type=ClusterIP \
              --set grafana.env.GF_SERVER_ROOT_URL='http://localhost/grafana/' \
              --set prometheus.service.type=ClusterIP \
              --set prometheus.prometheusSpec.externalUrl='http://localhost/prometheus/'\
              --set prometheus.prometheusSpec.routePrefix='/prometheus'
          displayName: 'Instalar Prometheus y Grafana en monitoring'


- stage: DeployLoggingStack
  displayName: 'Instalar ELK Stack Simple'
  dependsOn: DeployMonitoringStack
  condition: succeeded()
  jobs:
    - job: InstallELKSimple
      displayName: 'Instalar ELK Stack Simplificado'
      pool:
        vmImage: 'ubuntu-latest'
      steps:
        - task: HelmInstaller@1
          inputs:
            helmVersionToInstall: '3.13.3'

        - task: AzureCLI@2
          displayName: 'Login en AKS'
          inputs:
            azureSubscription: $(AZURE_ACCOUNT)
            scriptType: 'bash'
            scriptLocation: 'inlineScript'
            inlineScript: |
              az aks get-credentials --resource-group $(aksResourceGroup) --name $(aksClusterName) --overwrite-existing

        - script: |
            echo " Instalando ELK Stack simplificado..."

            kubectl create namespace logging --dry-run=client -o yaml | kubectl apply -f -

            helm uninstall elasticsearch -n logging --ignore-not-found
            helm uninstall kibana -n logging --ignore-not-found
            helm uninstall fluent-bit -n logging --ignore-not-found
            kubectl delete pvc -n logging --all --ignore-not-found

            helm repo add elastic https://helm.elastic.co
            helm repo add fluent https://fluent.github.io/helm-charts
            helm repo update

            helm install elasticsearch elastic/elasticsearch \
              --namespace logging \
              --set replicas=1 \
              --set minimumMasterNodes=1 \
              --set persistence.enabled=false \
              --set security.enabled=false \
              --set protocol=http \
              --set httpPort=9200 \
              --set transportPort=9300 \
              --set esJavaOpts="-Xms256m -Xmx256m" \
              --set clusterName="simple-es" \
              --set nodeGroup="master" \
              --set extraEnvs[0].name="discovery.type" \
              --set extraEnvs[0].value="single-node" \
              --set extraEnvs[1].name="xpack.security.enabled" \
              --set extraEnvs[1].value="false" \
              --timeout=10m \
              --wait

            kubectl wait --for=condition=ready pod -l app=elasticsearch-master -n logging --timeout=300s

            helm install kibana elastic/kibana \
              --namespace logging \
              --set elasticsearchHosts="http://elasticsearch-master:9200" \
              --set service.type=ClusterIP \
              --set service.port=5601 \
              --timeout=10m \
              --wait

            cat <<EOF > fluent-bit-config.yaml
            config:
              service: |
                [SERVICE]
                    Daemon Off
                    Flush 1
                    Log_Level info
                    Parsers_File parsers.conf
                    Parsers_File custom_parsers.conf
                    HTTP_Server On
                    HTTP_Listen 0.0.0.0
                    HTTP_Port 2020
                    Health_Check On

              inputs: |
                [INPUT]
                    Name tail
                    Path /var/log/containers/*.log
                    multiline.parser docker, cri
                    Tag kube.*
                    Mem_Buf_Limit 50MB
                    Skip_Long_Lines On

              filters: |
                [FILTER]
                    Name kubernetes
                    Match kube.*
                    Merge_Log On
                    Keep_Log Off
                    K8S-Logging.Parser On
                    K8S-Logging.Exclude On

              outputs: |
                [OUTPUT]
                    Name es
                    Match kube.*
                    Host elasticsearch-master.logging.svc.cluster.local
                    Port 9200
                    Index fluent-bit
                    Type _doc
                    Retry_Limit 3
                    Replace_Dots On
                    Logstash_Format On
                    Logstash_Prefix kubernetes
                    Logstash_DateFormat %Y.%m.%d

              customParsers: |
                [PARSER]
                    Name docker
                    Format json
                    Time_Key time
                    Time_Format %Y-%m-%dT%H:%M:%S.%L
                    Time_Keep On
            EOF

            helm install fluent-bit fluent/fluent-bit \
              --namespace logging \
              --values fluent-bit-config.yaml \
              --timeout=5m \
              --wait

            kubectl get pods -n logging -o wide
            kubectl get svc -n logging
          displayName: 'Instalar ELK Stack Simple'

        - script: |
            echo " Verificaci贸n post-instalaci贸n..."

            sleep 30

            kubectl get pods -n logging -o wide
            kubectl get svc -n logging
            kubectl exec -n logging deployment/elasticsearch-master -- curl -s http://localhost:9200/_cluster/health?pretty || echo "Elasticsearch a煤n iniciando..."
            kubectl exec -n logging deployment/elasticsearch-master -- curl -s http://localhost:9200/_cat/indices || echo "Sin 铆ndices a煤n..."
            kubectl get pods -n logging -l app.kubernetes.io/name=fluent-bit
          displayName: 'Verificaci贸n Post-Instalaci贸n'
