# Este pipeline se debe disparar manualmente.
trigger: none

pool:
  vmImage: ubuntu-latest

parameters:
- name: environment
  displayName: 'Select Environment'
  type: string
  default: 'dev'
  values:
  - dev
  - staging
  - prod

variables:
  - group: variable-group-taller
  - name: environmentPath
    value: 'terraform/environments/${{ parameters.environment }}'
  - name: aksResourceGroup
    value: '${{ parameters.environment }}-resource-group'
  - name: aksClusterName
    value: '${{ parameters.environment }}-aks-cluster'
  - name: kubernetesNamespace
    value: 'logging'


stages:
- stage: Terraform_plan_apply_${{ parameters.environment }}
  displayName: 'Terraform Plan & Apply - ${{ parameters.environment }}'
  # Etapa Ãºnica: Generar el plan y aplicar Terraform.
  jobs:
    - job: PlanAndApply
      continueOnError: false
      timeoutInMinutes: 30
      steps:
        # Paso 1: Instalar Terraform
        - task: TerraformInstaller@0
          inputs:
            terraformVersion: '1.5.7'
          displayName: 'Install Terraform'

        # Paso 2: Verificar versiÃ³n
        - script: terraform --version
          displayName: 'Check Terraform version'        
          # Paso 3: Verificar estructura de directorios
        - script: |
            echo "Working with environment: ${{ parameters.environment }}"
            echo "Environment path: $(environmentPath)"
            echo "Current directory: $(System.DefaultWorkingDirectory)"
            ls -la $(System.DefaultWorkingDirectory)/ || dir $(System.DefaultWorkingDirectory)\
            ls -la $(System.DefaultWorkingDirectory)/$(environmentPath)/ || dir $(System.DefaultWorkingDirectory)\$(environmentPath)\ || echo "Directory not found"
            echo "Files in environment directory:"
            ls -la $(System.DefaultWorkingDirectory)/$(environmentPath)/*.tf $(System.DefaultWorkingDirectory)/$(environmentPath)/*.tfvars 2>/dev/null || dir $(System.DefaultWorkingDirectory)\$(environmentPath)\*.tf $(System.DefaultWorkingDirectory)\$(environmentPath)\*.tfvars 2>nul || echo "No .tf or .tfvars files found"
          displayName: 'Check directory structure and files'
        # Paso 4: Terraform init
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'init'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            backendServiceArm: $(AZURE_ACCOUNT)
            backendAzureRmResourceGroupName: 'VM'
            backendAzureRmStorageAccountName: 'vmrecursos'
            backendAzureRmContainerName: 'tfstate'
            backendAzureRmKey: '${{ parameters.environment }}-aks.tfstate'
          displayName: 'Terraform init'

        # Paso 5: Terraform validate
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'validate'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
          displayName: 'Terraform validate'        # Paso 6: Terraform plan (usando task oficial)
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'plan'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            environmentServiceNameAzureRM: $(AZURE_ACCOUNT)
            commandOptions: '-out=plan.out -var-file=terraform.tfvars'
          displayName: 'Terraform Plan'

        # Paso 7: Publicar artefacto del plan
        - task: PublishPipelineArtifact@1
          inputs:
            targetPath: '$(System.DefaultWorkingDirectory)/$(environmentPath)/plan.out'
            artifact: 'Plan-${{ parameters.environment }}'
            publishLocation: 'pipeline'
          displayName: 'Publish Plan Artifact'
          condition: succeeded()

        # Paso 8: Terraform apply
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'apply'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            environmentServiceNameAzureRM: $(AZURE_ACCOUNT)
            commandOptions: '-auto-approve plan.out'
          displayName: 'Terraform Apply - ${{ parameters.environment }}'        # Paso 9: Mostrar outputs
        - task: TerraformTaskV4@4
          inputs:
            provider: 'azurerm'
            command: 'output'
            workingDirectory: '$(System.DefaultWorkingDirectory)/$(environmentPath)'
            environmentServiceNameAzureRM: $(AZURE_ACCOUNT)
          displayName: 'Show Terraform Outputs'
          continueOnError: true

        # Paso 10: Validar deployment
        - script: |
            echo "Deployment completed for environment: ${{ parameters.environment }}"
            echo "Validating deployment..."
            cd $(System.DefaultWorkingDirectory)/$(environmentPath)
            # Verificar estado de Terraform
            terraform show -json > terraform_state.json 2>/dev/null || echo "Could not export state"
            # Mostrar resumen de recursos creados
            terraform state list 2>/dev/null || echo "Could not list state"
            echo "Deployment validation completed"
          displayName: 'Validate Deployment'
          continueOnError: true

        # Paso 11: Cleanup temporal files
        - script: |
            cd $(System.DefaultWorkingDirectory)/$(environmentPath)
            # Limpiar archivos temporales pero conservar el estado
            rm -f plan.out terraform_state.json 2>/dev/null || del plan.out terraform_state.json 2>nul
            echo "Cleanup completed"
          displayName: 'Cleanup temporary files'
          continueOnError: true
          condition: always()

- stage: DeployMonitoringStack
  displayName: 'Instalar Prometheus & Grafana con Helm'
  dependsOn: Terraform_plan_apply_${{ parameters.environment }}
  condition: succeeded()
  jobs:
    - job: InstallMonitoring
      displayName: 'Helm Install kube-prometheus-stack'
      pool:
        vmImage: 'ubuntu-latest'
      steps:
        - task: HelmInstaller@1
          inputs:
            helmVersionToInstall: '3.13.3'
        - task: AzureCLI@2
          displayName: 'Login en AKS'
          inputs:
            azureSubscription:  $(AZURE_ACCOUNT)  # Debe estar configurado en Azure DevOps
            scriptType: 'bash'
            scriptLocation: 'inlineScript'
            inlineScript: |
              az aks get-credentials --resource-group $(aksResourceGroup) --name $(aksClusterName) --overwrite-existing

        - script: |
            echo "ğŸ”§ Crear namespace monitoring si no existe"
            kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -

            echo "ğŸ“¦ Agregar repositorio Helm de Prometheus"
            helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
            helm repo update

            echo "ğŸš€ Instalar kube-prometheus-stack con configuraciÃ³n para Nginx"
            helm upgrade --install kube-prometheus-stack prometheus-community/kube-prometheus-stack \
              --namespace monitoring \
              --set grafana.adminPassword='MiPassword123' \
              --set grafana.service.type=ClusterIP \
              --set grafana.env.GF_SERVER_ROOT_URL='http://localhost/grafana/' \
              --set prometheus.service.type=ClusterIP \
              --set prometheus.prometheusSpec.externalUrl='http://localhost/prometheus/'\
              --set prometheus.prometheusSpec.routePrefix='/prometheus'
          displayName: 'Instalar Prometheus y Grafana en monitoring'


- stage: InstallELKStack
  displayName: 'Instalar ELK Stack completo en un solo stage'
  jobs:
    - job: DeployELK
      displayName: 'Deploy Elasticsearch, Kibana y Fluent Bit'
      pool:
        vmImage: 'ubuntu-latest'
      steps:
        - task: AzureCLI@2
          displayName: 'Autenticarse en Azure y desplegar ELK'
          inputs:
            azureSubscription:  $(AZURE_ACCOUNT)
            scriptType: bash
            scriptLocation: inlineScript
            inlineScript: |
              echo "ğŸ” Autenticando en Azure y configurando AKS..."
              az aks get-credentials --resource-group $(aksResourceGroup) --name $(aksClusterName) --overwrite-existing

              echo "ğŸ”§ AÃ±adiendo repositorios Helm..."
              helm repo add elastic https://helm.elastic.co || true
              helm repo add fluent https://fluent.github.io/helm-charts || true
              helm repo update

              echo "ğŸ§¹ Eliminando posibles instalaciones previas..."
              helm uninstall elasticsearch -n $(kubernetesNamespace) || true
              helm uninstall kibana -n $(kubernetesNamespace) || true
              helm uninstall fluent-bit -n $(kubernetesNamespace) || true

              echo "ğŸš€ Creando namespace si no existe..."
              kubectl create namespace $(kubernetesNamespace) --dry-run=client -o yaml | kubectl apply -f -                 
              echo "ğŸ“¦ Instalando Elasticsearch sin autenticaciÃ³n (configuraciÃ³n avanzada)..."
              helm upgrade --install elasticsearch elastic/elasticsearch \
                --namespace $(kubernetesNamespace) \
                --set replicas=1 \
                --set minimumMasterNodes=1 \
                --set resources.requests.cpu="100m" \
                --set resources.requests.memory="512Mi" \
                --set resources.limits.cpu="500m" \
                --set resources.limits.memory="1Gi" \
                --set secret.enabled=false \
                --set createCert=false \
                --set esConfig."elasticsearch\.yml"="xpack.security.enabled: false\nxpack.security.transport.ssl.enabled: false\nxpack.security.http.ssl.enabled: false\nxpack.monitoring.collection.enabled: false\nxpack.security.enrollment.enabled: false\ndiscovery.type: single-node\nnetwork.host: 0.0.0.0" \
                --set readinessProbe.initialDelaySeconds=60 \
                --set readinessProbe.periodSeconds=30 \
                --set readinessProbe.timeoutSeconds=30 \
                --set readinessProbe.exec.command="['sh','-c','curl -f http://localhost:9200/_cluster/health?local=true']" \
                --wait

              echo "ğŸ“Š Instalando Kibana sin autenticaciÃ³n..."
              helm upgrade --install kibana elastic/kibana \
                --namespace $(kubernetesNamespace) \
                --set service.type=ClusterIP \
                --set elasticsearchHosts="http://elasticsearch-master:9200" \
                --set kibanaConfig."kibana\.yml"="elasticsearch.hosts: ['http://elasticsearch-master:9200']\nxpack.security.enabled: false\nxpack.encryptedSavedObjects.encryptionKey: 'min-32-char-long-strong-encryption-key'" \
                --set extraEnvs[0].name=ELASTICSEARCH_USERNAME \
                --set extraEnvs[0].value="" \
                --set extraEnvs[1].name=ELASTICSEARCH_PASSWORD \
                --set extraEnvs[1].value="" \
                --wait              echo "ğŸ”¥ Instalando Fluent Bit con configuraciÃ³n HTTP simplificada..."
              helm upgrade --install fluent-bit fluent/fluent-bit \
                --namespace $(kubernetesNamespace) \
                --set env[0].name=FLUENT_ELASTICSEARCH_HOST \
                --set env[0].value=elasticsearch-master \
                --set env[1].name=FLUENT_ELASTICSEARCH_PORT \
                --set env[1].value=9200 \
                --set config.outputs="[OUTPUT]\n    Name es\n    Match *\n    Host elasticsearch-master\n    Port 9200\n    HTTP_User \"\"\n    HTTP_Passwd \"\"\n    tls Off\n    tls.verify Off\n    Logstash_Format On\n    Suppress_Type_Name On\n    Retry_Limit False" \
                --wait

              echo "â³ Esperando 30 segundos para que Elasticsearch estÃ© completamente listo..."
              sleep 30

              echo "âœ… ELK Stack instalado correctamente en el namespace '$(kubernetesNamespace)'"              echo "ğŸ” Verificando estado de los pods..."
              kubectl get pods -n $(kubernetesNamespace)

              echo "â³ Esperando a que todos los pods estÃ©n listos..."
              kubectl wait --for=condition=ready pod -l app=elasticsearch-master -n $(kubernetesNamespace) --timeout=600s || echo "âš ï¸ Elasticsearch tardÃ³ mÃ¡s de lo esperado"
              kubectl wait --for=condition=ready pod -l app=kibana -n $(kubernetesNamespace) --timeout=300s || echo "âš ï¸ Kibana tardÃ³ mÃ¡s de lo esperado"
              kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=fluent-bit -n $(kubernetesNamespace) --timeout=180s || echo "âš ï¸ Fluent Bit tardÃ³ mÃ¡s de lo esperado"

              echo "ğŸ§ª Probando conectividad a Elasticsearch..."
              for i in {1..10}; do
                if kubectl exec -n $(kubernetesNamespace) deployment/elasticsearch-master -- curl -f -s http://localhost:9200 > /dev/null 2>&1; then
                  echo "âœ… Elasticsearch estÃ¡ respondiendo!"
                  break
                else
                  echo "â³ Intento $i/10: Elasticsearch aÃºn no responde, esperando 30s..."
                  sleep 30
                fi
              done

              echo "ğŸ“Š Verificando Ã­ndices en Elasticsearch..."
              kubectl exec -n $(kubernetesNamespace) deployment/elasticsearch-master -- curl -s "http://localhost:9200/_cat/indices?v" || echo "âŒ No se pudieron obtener Ã­ndices"

              echo "ğŸ”§ DiagnÃ³stico de problemas..."
              echo "--- Estado de pods ---"
              kubectl get pods -n $(kubernetesNamespace) -o wide
              echo "--- Eventos recientes ---"
              kubectl get events -n $(kubernetesNamespace) --sort-by='.lastTimestamp' | tail -10
              echo "--- Logs de Elasticsearch (Ãºltimas 10 lÃ­neas) ---"
              kubectl logs -n $(kubernetesNamespace) -l app=elasticsearch-master --tail=10 || echo "No se pudieron obtener logs de Elasticsearch"
              echo "--- Logs de Fluent Bit (Ãºltimas 5 lÃ­neas) ---"
              kubectl logs -n $(kubernetesNamespace) -l app.kubernetes.io/name=fluent-bit --tail=5 || echo "No se pudieron obtener logs de Fluent Bit"

              echo "ğŸ‰ InstalaciÃ³n del ELK Stack completada!"
              echo "ğŸ“ Para acceder a Kibana, usa: kubectl port-forward -n $(kubernetesNamespace) svc/kibana-kibana 5601:5601"
              echo "ğŸ” Para ver logs de Fluent Bit: kubectl logs -n $(kubernetesNamespace) -l app.kubernetes.io/name=fluent-bit"
